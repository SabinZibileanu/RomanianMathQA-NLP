{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T11:23:41.410875Z",
     "iopub.status.busy": "2025-04-25T11:23:41.410710Z",
     "iopub.status.idle": "2025-04-25T11:25:01.252811Z",
     "shell.execute_reply": "2025-04-25T11:25:01.252066Z",
     "shell.execute_reply.started": "2025-04-25T11:23:41.410859Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install datasets bitsandbytes peft trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T11:25:54.256095Z",
     "iopub.status.busy": "2025-04-25T11:25:54.255371Z",
     "iopub.status.idle": "2025-04-25T11:25:54.261254Z",
     "shell.execute_reply": "2025-04-25T11:25:54.260443Z",
     "shell.execute_reply.started": "2025-04-25T11:25:54.256034Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T11:50:58.090527Z",
     "iopub.status.busy": "2025-04-25T11:50:58.089826Z",
     "iopub.status.idle": "2025-04-25T11:50:58.095021Z",
     "shell.execute_reply": "2025-04-25T11:50:58.094276Z",
     "shell.execute_reply.started": "2025-04-25T11:50:58.090502Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "from huggingface_hub import login\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from peft import LoraConfig, get_peft_model, AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "import torch\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "from sklearn.metrics import f1_score\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T11:26:39.160570Z",
     "iopub.status.busy": "2025-04-25T11:26:39.159894Z",
     "iopub.status.idle": "2025-04-25T11:26:39.239794Z",
     "shell.execute_reply": "2025-04-25T11:26:39.239306Z",
     "shell.execute_reply.started": "2025-04-25T11:26:39.160543Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "login(\"\") # Insert HF API key here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T11:26:40.662480Z",
     "iopub.status.busy": "2025-04-25T11:26:40.661543Z",
     "iopub.status.idle": "2025-04-25T11:26:40.912429Z",
     "shell.execute_reply": "2025-04-25T11:26:40.911650Z",
     "shell.execute_reply.started": "2025-04-25T11:26:40.662449Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the datasets. For this work, we used the Bac subset and the Comps subset from RoMath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T11:27:12.079621Z",
     "iopub.status.busy": "2025-04-25T11:27:12.078932Z",
     "iopub.status.idle": "2025-04-25T11:27:14.879471Z",
     "shell.execute_reply": "2025-04-25T11:27:14.878927Z",
     "shell.execute_reply.started": "2025-04-25T11:27:12.079595Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ro_math_bac_set = load_dataset('cosmadrian/romath', 'bac')\n",
    "ro_math_comp_set = load_dataset('cosmadrian/romath', 'comps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining some global variables:\n",
    "- Mathematical common latex tokens that will help us later\n",
    "- The prompts for the model corresponding to their subsets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T11:50:12.477996Z",
     "iopub.status.busy": "2025-04-25T11:50:12.477369Z",
     "iopub.status.idle": "2025-04-25T11:50:12.481832Z",
     "shell.execute_reply": "2025-04-25T11:50:12.481110Z",
     "shell.execute_reply.started": "2025-04-25T11:50:12.477977Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "MATHEMATICAL_TOKENS = [\n",
    "      \"\\\\frac\", \"\\\\sqrt\", \"\\\\lim\", \"\\\\int\", \"\\\\sum\", \"\\\\prod\",\n",
    "      \"\\\\sin\", \"\\\\cos\", \"\\\\tan\", \"\\\\log\", \"\\\\ln\", \"\\\\cdot\",\n",
    "      \"\\\\rightarrow\", \"\\\\to\", \"\\\\leq\", \"\\\\geq\", \"\\\\neq\",\n",
    "      \"\\\\in\", \"\\\\approx\", \"\\\\mathbf{R}\", \"\\\\mathbb{R}\", \"\\\\pi\",\n",
    "      \"^\", \"_\", \"\\\\left\", \"\\\\right\", \"\\\\operatorname\", \"=\"\n",
    "  ]\n",
    "\n",
    "baccalaureate_prompt = \"Ești un elev în clasa a 12-a care se pregătește pentru examenul de Bacalaureat la matematică. Analizează foarte bine întrebarea și răspunde la exercițiile următoare oferind doar răspunsul final, fără explicații suplimentare. Dacă este cazul, folosește notații matematice în LaTeX pentru a scrie corect rezultatul.\"\n",
    "competition_prompt = \"Ești un elev pasionat de matematică, antrenat pentru olimpiade și concursuri naționale. Analizează foarte bine întrebarea și răspunde la exercițiile următoare oferind doar răspunsul final, fără explicații suplimentare. Dacă este cazul, folosește notații matematice în LaTeX pentru a scrie corect rezultatul.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing some basic exploratory data analysis: \n",
    "- Visualizing the problems domain distribution across each subset\n",
    "- Visualizing the distribution of the common mathematical tokens across each subset\n",
    "\n",
    "\n",
    "To do that, we combined the training and test splits to get a better understanding of the distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T11:49:56.272016Z",
     "iopub.status.busy": "2025-04-25T11:49:56.271719Z",
     "iopub.status.idle": "2025-04-25T11:49:56.284530Z",
     "shell.execute_reply": "2025-04-25T11:49:56.283996Z",
     "shell.execute_reply.started": "2025-04-25T11:49:56.271994Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ro_math_bac_set_combined = concatenate_datasets([ro_math_bac_set['train'], ro_math_bac_set['test']])\n",
    "ro_math_comp_set_combined = concatenate_datasets([ro_math_comp_set['train'], ro_math_comp_set['test']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T11:49:57.850132Z",
     "iopub.status.busy": "2025-04-25T11:49:57.849818Z",
     "iopub.status.idle": "2025-04-25T11:49:57.856513Z",
     "shell.execute_reply": "2025-04-25T11:49:57.855880Z",
     "shell.execute_reply.started": "2025-04-25T11:49:57.850098Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DatasetAnalyzer:\n",
    "  \"\"\"\n",
    "    A class that helps with EDA.\n",
    "    Currently supports:\n",
    "    - Computing domain frequency distributions for each dataset subset (bac, comps).\n",
    "    - Computing the distribution of special LaTex mathematical tokens\n",
    "  \"\"\"\n",
    "\n",
    "  MATHEMATICAL_PATTERN_LATEX = re.compile(r\"(\\\\[a-zA-Z]+|[=_^])\")\n",
    "  \n",
    "  DOMAIN_COLUMN_NAME = 'domain'\n",
    "  PROBLEM_COLUMN_NAME = 'problem'\n",
    "\n",
    "\n",
    "  def __init__(self, ro_math_bac_set, ro_math_comp_set):\n",
    "    self.data_subsets = {\n",
    "        'bac': ro_math_bac_set,\n",
    "        'comps': ro_math_comp_set\n",
    "    }\n",
    "\n",
    "    self.math_latex_tokens = MATHEMATICAL_TOKENS\n",
    "\n",
    "\n",
    "  def get_domain_distribution(self, subset: str):\n",
    "    if subset not in self.data_subsets:\n",
    "      raise ValueError(\"Invalid data subset\")\n",
    "\n",
    "    analyzed_subset = self.data_subsets[subset]\n",
    "    return dict(Counter(analyzed_subset[self.DOMAIN_COLUMN_NAME]))\n",
    "  \n",
    "  def get_mathematical_tokens_distribution(self, subset: str):\n",
    "    if subset not in self.data_subsets:\n",
    "      raise ValueError(\"Invalid data subset\")\n",
    "\n",
    "    analyzed_subset = self.data_subsets[subset][self.PROBLEM_COLUMN_NAME]\n",
    "    math_tokens_frequency = {key: 0 for key in self.math_latex_tokens}\n",
    "\n",
    "    for problem in analyzed_subset:\n",
    "      math_symbols = self.MATHEMATICAL_PATTERN_LATEX.findall(problem)\n",
    "      \n",
    "      for symbol in math_symbols:\n",
    "        if symbol in self.math_latex_tokens:\n",
    "          math_tokens_frequency[symbol] += 1\n",
    "    \n",
    "    return math_tokens_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T11:50:00.030287Z",
     "iopub.status.busy": "2025-04-25T11:50:00.029998Z",
     "iopub.status.idle": "2025-04-25T11:50:00.040347Z",
     "shell.execute_reply": "2025-04-25T11:50:00.039706Z",
     "shell.execute_reply.started": "2025-04-25T11:50:00.030267Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DatasetVisualizer(DatasetAnalyzer):\n",
    "\n",
    "  \"\"\"\n",
    "    Inherits from DatasetAnalyzer. Adds visualization methods for domain and mathematical tokens distributions\n",
    "    across BAC and COMP subsets.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, ro_math_bac_set, ro_math_comp_set):\n",
    "    super().__init__(ro_math_bac_set, ro_math_comp_set)\n",
    "\n",
    "  def visualize_domain_distributions(self):\n",
    "    overall_domain_distributions = [self.get_domain_distribution(subset) for subset in self.data_subsets]\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(18, 5))\n",
    "    bar_width = 0.6\n",
    "    plot_colors = ['b', 'g']\n",
    "\n",
    "    for idx, (subset, color) in enumerate(zip(self.data_subsets, plot_colors)):\n",
    "      indices_x_axis = np.arange(len(overall_domain_distributions[idx]))\n",
    "      labels = list(overall_domain_distributions[idx].keys())\n",
    "      max_upper_bound_domain = max(overall_domain_distributions[idx].values())\n",
    "\n",
    "      ax[idx].bar(indices_x_axis, overall_domain_distributions[idx].values(), color = color, width = bar_width)\n",
    "      ax[idx].set_title(subset.upper() + ' domain distribution')\n",
    "      ax[idx].set_xticks(indices_x_axis)\n",
    "      ax[idx].set_xticklabels(labels, rotation = 70)\n",
    "      ax[idx].set_ylim(0, max_upper_bound_domain * 1.1)\n",
    "      ax[idx].set_ylabel('Domain counter')\n",
    "\n",
    "    fig.suptitle(\"Domain Distribution Across Subsets\", fontsize=16)\n",
    "    plt.show()\n",
    "  \n",
    "\n",
    "  def visualize_token_distributions(self):\n",
    "    overall_token_distributions = [self.get_mathematical_tokens_distribution(subset) for subset in self.data_subsets]\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(18, 5))\n",
    "    bar_width = 0.6\n",
    "    plot_colors = ['b', 'g']\n",
    "\n",
    "    for idx, (subset, color) in enumerate(zip(self.data_subsets, plot_colors)):\n",
    "      indices_x_axis = np.arange(len(overall_token_distributions[idx]))\n",
    "      labels = list(overall_token_distributions[idx].keys())\n",
    "      max_upper_bound_domain = max(overall_token_distributions[idx].values())\n",
    "\n",
    "      ax[idx].bar(indices_x_axis, overall_token_distributions[idx].values(), color = color, width = bar_width)\n",
    "      ax[idx].set_title(subset.upper() + ' mathematical tokens distribution')\n",
    "      ax[idx].set_xticks(indices_x_axis)\n",
    "      ax[idx].set_xticklabels(labels, rotation = 70)\n",
    "      ax[idx].set_ylim(0, max_upper_bound_domain * 1.1)\n",
    "      ax[idx].set_ylabel('Mathematical tokens counter')\n",
    "\n",
    "    fig.suptitle(\"Mathematical tokens distribution across subsets\", fontsize=16)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T11:50:40.916470Z",
     "iopub.status.busy": "2025-04-25T11:50:40.915811Z",
     "iopub.status.idle": "2025-04-25T11:50:41.813863Z",
     "shell.execute_reply": "2025-04-25T11:50:41.813105Z",
     "shell.execute_reply.started": "2025-04-25T11:50:40.916447Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset_visualizer = DatasetVisualizer(ro_math_bac_set_combined, ro_math_comp_set_combined)\n",
    "dataset_visualizer.visualize_domain_distributions()\n",
    "dataset_visualizer.visualize_token_distributions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the model and the tokenizer.\n",
    "\n",
    "For a better memory usage we quantized the model to 4bit.\n",
    "\n",
    "We narrowed the LLM choice down to Ro-Llama and Ro-Mistral in the instruct format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T11:53:58.716720Z",
     "iopub.status.busy": "2025-04-25T11:53:58.716426Z",
     "iopub.status.idle": "2025-04-25T11:55:18.881998Z",
     "shell.execute_reply": "2025-04-25T11:55:18.881438Z",
     "shell.execute_reply.started": "2025-04-25T11:53:58.716700Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(model_name):\n",
    "  bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16)\n",
    "\n",
    "  model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config)\n",
    "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "  return model, tokenizer\n",
    "\n",
    "# model, tokenizer = load_model_and_tokenizer(\"OpenLLM-Ro/RoLlama2-7b-Instruct\")\n",
    "model, tokenizer = load_model_and_tokenizer(\"OpenLLM-Ro/RoMistral-7b-Instruct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now this is where the mathematical tokens come into play. Our idea is to add those mathematical tokens into the pre-trained tokenizer and then extend the model token embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T11:59:51.170328Z",
     "iopub.status.busy": "2025-04-25T11:59:51.169998Z",
     "iopub.status.idle": "2025-04-25T11:59:52.535379Z",
     "shell.execute_reply": "2025-04-25T11:59:52.534781Z",
     "shell.execute_reply.started": "2025-04-25T11:59:51.170304Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tokens_to_add = [token for token in MATHEMATICAL_TOKENS if token not in tokenizer.get_vocab()]\n",
    "tokenizer.add_tokens(tokens_to_add)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of how it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T12:04:23.126638Z",
     "iopub.status.busy": "2025-04-25T12:04:23.125925Z",
     "iopub.status.idle": "2025-04-25T12:04:23.131655Z",
     "shell.execute_reply": "2025-04-25T12:04:23.131128Z",
     "shell.execute_reply.started": "2025-04-25T12:04:23.126613Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tokenizer.tokenize('Să se calculeze \\(\\sin 2 \\pi+\\sin 4 \\pi\\).')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formatting the dataset with the proper question-answering format. As an answer, we will stick to using the short answer to the problem, as reasoning for now is a little bit hard to do without any human supervision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T12:05:40.062415Z",
     "iopub.status.busy": "2025-04-25T12:05:40.062145Z",
     "iopub.status.idle": "2025-04-25T12:05:40.067628Z",
     "shell.execute_reply": "2025-04-25T12:05:40.066946Z",
     "shell.execute_reply.started": "2025-04-25T12:05:40.062396Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def format_questions_answers(data_samples, llm_prompt):\n",
    "  EOS_TOKEN = tokenizer.eos_token\n",
    "\n",
    "  questions_column = data_samples['problem']\n",
    "  answers_column = data_samples['answer']\n",
    "  \n",
    "\n",
    "  formatted_texts = []\n",
    "\n",
    "  for question, answer in zip(questions_column, answers_column):\n",
    "    full_formatted_answer = f'Răspunsul este {answer.strip()}'\n",
    "\n",
    "    messages_ro_llm = [\n",
    "      {\"role\": \"system\", \"content\": llm_prompt},\n",
    "      {\"role\": \"user\", \"content\": question},\n",
    "      {\"role\": \"assistant\", \"content\": full_formatted_answer + EOS_TOKEN}\n",
    "    ]\n",
    "\n",
    "    formatted_message = tokenizer.apply_chat_template(\n",
    "        messages_ro_llm,\n",
    "        tokenize = False,\n",
    "        add_generation_prompt = False\n",
    "    )\n",
    "\n",
    "    formatted_texts.append(formatted_message)\n",
    "\n",
    "  return {\"text\": formatted_texts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T12:05:43.495977Z",
     "iopub.status.busy": "2025-04-25T12:05:43.495709Z",
     "iopub.status.idle": "2025-04-25T12:05:43.821195Z",
     "shell.execute_reply": "2025-04-25T12:05:43.820505Z",
     "shell.execute_reply.started": "2025-04-25T12:05:43.495956Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ro_math_bac_train = ro_math_bac_set['train']\n",
    "ro_math_bac_eval = ro_math_bac_set['test']\n",
    "\n",
    "ro_math_bac_train_updated = ro_math_bac_train.map(lambda x: format_questions_answers(x, baccalaureate_prompt), batched = True)\n",
    "ro_math_bac_eval_updated = ro_math_bac_eval.map(lambda x: format_questions_answers(x, baccalaureate_prompt), batched = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T12:05:45.626575Z",
     "iopub.status.busy": "2025-04-25T12:05:45.625850Z",
     "iopub.status.idle": "2025-04-25T12:05:45.768784Z",
     "shell.execute_reply": "2025-04-25T12:05:45.768016Z",
     "shell.execute_reply.started": "2025-04-25T12:05:45.626552Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ro_math_comp_train = ro_math_comp_set['train']\n",
    "ro_math_comp_eval = ro_math_comp_set['test']\n",
    "\n",
    "ro_math_comp_train_updated = ro_math_comp_train.map(lambda x: format_questions_answers(x, competition_prompt), batched = True)\n",
    "ro_math_comp_eval_updated = ro_math_comp_eval.map(lambda x: format_questions_answers(x, competition_prompt), batched = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T12:05:49.227914Z",
     "iopub.status.busy": "2025-04-25T12:05:49.227672Z",
     "iopub.status.idle": "2025-04-25T12:05:49.238993Z",
     "shell.execute_reply": "2025-04-25T12:05:49.238410Z",
     "shell.execute_reply.started": "2025-04-25T12:05:49.227888Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ro_math_bac_train_updated = ro_math_bac_train_updated.shuffle(seed = 42)\n",
    "ro_math_comp_train_updated = ro_math_comp_train_updated.shuffle(seed = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Low Rank Adaptation for fine tuning the models\n",
    "\n",
    "For the changes, we applied the bias to lora_only allowing the biases to adapt during fine tuning\n",
    "A dropout of 0.2 to introduce regularization and prevent overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T12:09:44.636398Z",
     "iopub.status.busy": "2025-04-25T12:09:44.636005Z",
     "iopub.status.idle": "2025-04-25T12:09:44.640563Z",
     "shell.execute_reply": "2025-04-25T12:09:44.639938Z",
     "shell.execute_reply.started": "2025-04-25T12:09:44.636376Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "                          lora_alpha=16,\n",
    "                          lora_dropout=0.2,\n",
    "                          r=32,\n",
    "                          bias=\"lora_only\",\n",
    "                          task_type=\"CAUSAL_LM\",\n",
    "                          target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "    ]\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T12:10:55.866781Z",
     "iopub.status.busy": "2025-04-25T12:10:55.866251Z",
     "iopub.status.idle": "2025-04-25T12:10:56.981397Z",
     "shell.execute_reply": "2025-04-25T12:10:56.980678Z",
     "shell.execute_reply.started": "2025-04-25T12:10:55.866760Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    processing_class = tokenizer,\n",
    "    train_dataset = ro_math_bac_train_updated,\n",
    "    eval_dataset = ro_math_bac_eval_updated,\n",
    "    peft_config = peft_config,\n",
    "    args = SFTConfig(\n",
    "        per_device_train_batch_size = 1,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        max_steps = 600,\n",
    "        max_seq_length = 384, # use this for the comp dataset as the problem instructions are quite long\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = True,\n",
    "        bf16 = False,\n",
    "        logging_steps = 100,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"output_math_qa_results\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T12:11:00.488123Z",
     "iopub.status.busy": "2025-04-25T12:11:00.487566Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='112' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [112/600 23:25 < 1:43:57, 0.08 it/s, Epoch 0.21/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.382800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T10:09:14.773646Z",
     "iopub.status.busy": "2025-04-25T10:09:14.773047Z",
     "iopub.status.idle": "2025-04-25T10:09:14.777107Z",
     "shell.execute_reply": "2025-04-25T10:09:14.776474Z",
     "shell.execute_reply.started": "2025-04-25T10:09:14.773623Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def cleanup():\n",
    "    \"Free up GPU memory\"\n",
    "    \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T10:09:16.823791Z",
     "iopub.status.busy": "2025-04-25T10:09:16.823199Z",
     "iopub.status.idle": "2025-04-25T10:09:17.423959Z",
     "shell.execute_reply": "2025-04-25T10:09:17.423352Z",
     "shell.execute_reply.started": "2025-04-25T10:09:16.823766Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compute_llm_eval_answers(eval_set, chat_instruction, model_ftuned, tokenizer_ftuned):\n",
    "    llm_clean_answers = []\n",
    "    \n",
    "    for _, problem_instruction in enumerate(tqdm(eval_set)):\n",
    "        chat = [\n",
    "            {\"role\": \"system\", \"content\": chat_instruction},\n",
    "            {\"role\": \"user\", \"content\": problem_instruction}\n",
    "        ]\n",
    "\n",
    "        \n",
    "        prompt = tokenizer_ftuned.apply_chat_template(chat, tokenize = False)\n",
    "        inputs = tokenizer_ftuned.encode(prompt, add_special_tokens=False, return_tensors=\"pt\").to(model_ftuned.device)\n",
    "        outputs = model_ftuned.generate(input_ids = inputs, max_new_tokens = 64)\n",
    "\n",
    "        out = tokenizer_ftuned.decode(outputs[0])\n",
    "        clean_answer = out.split('[/INST]')[1]\n",
    "        clean_answer = clean_answer.replace('</s>', '').strip()\n",
    "\n",
    "        llm_clean_answers.append(clean_answer)\n",
    "\n",
    "    return llm_clean_answers\n",
    "\n",
    "llm_clean = compute_llm_eval_answers(ro_math_bac_eval_updated['problem'], baccalaureate_prompt, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T11:13:40.826468Z",
     "iopub.status.busy": "2025-04-25T11:13:40.826209Z",
     "iopub.status.idle": "2025-04-25T11:13:40.831295Z",
     "shell.execute_reply": "2025-04-25T11:13:40.830714Z",
     "shell.execute_reply.started": "2025-04-25T11:13:40.826452Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compute_predictions_encoding(llm_ans, ground_truth_ans):\n",
    "    predictions_encoding = []\n",
    "    for llm_answer, ground_truth_answer in zip(llm_ans, ground_truth_ans):\n",
    "        ground_truth_answer_clean = ground_truth_answer.split('[/INST]')[1]\n",
    "        ground_truth_answer_clean = ground_truth_answer_clean.replace('</s>', '').strip()\n",
    "\n",
    "        predictions_encoding.append(1) if llm_answer == ground_truth_answer_clean else predictions_encoding.append(0)\n",
    "        \n",
    "\n",
    "    return predictions_encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T11:18:29.336580Z",
     "iopub.status.busy": "2025-04-25T11:18:29.336118Z",
     "iopub.status.idle": "2025-04-25T11:18:29.354481Z",
     "shell.execute_reply": "2025-04-25T11:18:29.353953Z",
     "shell.execute_reply.started": "2025-04-25T11:18:29.336551Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "encoded_predictions = compute_predictions_encoding(llm_clean, ro_math_bac_eval_updated['text'])\n",
    "encoded_ground_truth = [1] * len(ro_math_bac_eval_updated)\n",
    "\n",
    "f1_score(encoded_ground_truth, encoded_predictions)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
